---
title: Scientific Thinking
pubDate: 2025-11-09
---

## Part 1: The Glitch in Your Brain (Perception, Reality, and the Evolutionary Trap)

Science usually starts with a simple itch: curiosity. It’s that moment where we just want to know something. But simply wanting to know isn’t enough. Scientific thinking is the specific discipline of channeling that raw curiosity into a structure that produces reliable, load-bearing knowledge. It is the operating system we use to figure out new things about life, the universe, and everything else.

To understand why this specific "operating system" is necessary, we have to look at the relationship between three heavy concepts: Reality, Truth, and Knowledge. It sounds straightforward, but it gets messy immediately. For science to work, we generally have to assume that there is a stable, secured **Reality** out there and that there is a specific **Truth** about that reality that we can discover. That seems like an acceptable baseline. However, we have to acknowledge that this is an assumption. We could, theoretically, be living in a computer simulation. If we are, the term "Reality" becomes completely meaningless, and "Truth" becomes something else entirely. This isn't just high-grade philosophical weed-talk; philosopher Nick Bostrom formalized this in 2003 with his paper _Are you living in a computer simulation?_, and modern physics actually grapples with similar questions constantly.

But let’s put the simulation theory on the back burner for a moment. Even if we assume there is a single, physical reality, we run into a massive problem: Can we actually know the truth about it? If truth is unreachable, why do we assume reality exists at all? Before we get lost in epistemology, we need to audit the only tool we have for observing this reality: the human brain.

### The Conjecture: Our Hardware is Compromised

To test our ability to observe reality, we are going to run three experiments. A good experiment starts with a conjecture—a guess about what will happen. Our conjecture for this entire section is this: **Our perception and our thinking are not capable of conveying reliable information about the world.**

For the purposes of this discussion, we aren't drawing a sharp line between "perception" (the sensory input) and "thinking" (the processing). Whether the error happens in the eye or the cortex doesn't matter; we are interested in the reliability of the system as a whole. This aligns with the "Critical Thinking" models where we treat the human cognitive stack as a single, often flawed, unit.

### Experiment 1: The Blind Spot

Let's look at the human eye. It is, frankly, a bit of a design disaster. In humans, the nerve fibers run _in front_ of the retina. For those nerves to exit the eye and travel to the brain, they have to punch a hole through the retina. Where that hole exists, there are no photoreceptors. No rods, no cones, nothing. This is the **blind spot**. It is a structural failure in our vision. Interestingly, this wasn't inevitable; cephalopods (like octopuses) have nerve fibers running behind the retina, meaning they have no blind spot and get better light throughput. We got the short end of the evolutionary stick here.

You can verify this glitch right now. Imagine a black dot and a black "X" on a piece of paper, separated by about 10 centimeters. If you close your left eye and stare directly at the dot with your right eye, the "X" is in your peripheral vision. If you move your head back and forth—usually finding the sweet spot around 20 centimeters away—the "X" will suddenly vanish. It is gone. The image of the "X" is hitting exactly where your optic nerve punches through the retina.

But here is where it gets terrifying. Keep staring at the dot. Don't look away. Try to perceive what is in the empty space where the "X" used to be. You do not see a black void. You do not see a hole. If the background is white, you see white. If you run this experiment on a page full of text, replacing the "X" with a word, something even stranger happens. When the word hits your blind spot, you don't just see a gap. Your brain fills it in with "text." It is blurry, unreadable, and logically invalid text, but your brain generates the texture of writing to patch the hole.

This is a biological version of the "Content-Aware Fill" or "Inpainting" found in modern image editing software. Your brain knows there is no data there, but it refuses to show you the lack of data. It constructs a lie—a plausible texture—and presents it to you as reality. You are seeing something that you know, with 100% certainty, does not exist.

### Experiment 2: The Checker Shadow Illusion

We can dismiss the blind spot as a mechanical error, but the processing errors are worse. Consider the famous Checker Shadow Illusion published by neuroscientist Edward Adelson in 1995. The image depicts a 3D checkerboard with light and dark squares, with a green cylinder casting a shadow over the board. There is a dark square outside the shadow labeled "A" and a light square inside the shadow labeled "B".

Here is the kicker: Square A and Square B are exactly the same shade of grey. If you use a color picker tool or physically print it out and fold the paper to touch them together, they are identical pixel values.

But you cannot see it. No matter how hard you concentrate, your brain refuses to let you see them as the same color. This happens because your visual system is not a physical light meter. Its job is not to report the raw data of photons hitting the retina. Its job is to break image information down into meaningful components to identify objects. Your brain sees the shadow, understands how shadows work (they darken surfaces), and automatically "brightens" the interpretation of square B to help you identify it as a "light check" on the board.

Adelson argues this demonstrates the _success_ of the visual system, not its failure, because it allows us to identify objects regardless of lighting conditions. But for a scientist trying to observe raw data? It is a catastrophic failure. Our thinking refuses to acknowledge the truth (that the grey values are equal) in favor of a useful story (that one is a white square in a shadow). We are incapable of visually verifying the truth.

### Experiment 3: The Rubber Hand Illusion

If vision is unreliable, surely we can trust our own bodies? Enter the Rubber Hand Illusion. In this experiment, a participant sits with their real hand hidden behind a screen. A fake, often crude, rubber hand is placed on the table in front of them. The experimenter then strokes both the hidden real hand and the visible rubber hand simultaneously with a brush.

After a short period of synchronous stroking, the participant experiences **proprioceptive drift**. Their sense of body ownership shifts. They start to "feel" the brush on the rubber hand. The brain decides that the visual input (seeing the rubber hand being touched) is more important than the proprioceptive data (knowing where your real hand is).

This works even if the rubber hand doesn't look particularly realistic. If the experimenter suddenly smashes the rubber hand with a hammer, the participant will flinch in genuine fear. We can trick the fundamental sense of "self" and body location with a few cheap parlor tricks. Our rational mind knows it is plastic, but our sensory processing overrides that knowledge completely.

### The Evolutionary Trap: The Cognitive Miser

Let’s return to our conjecture: "Our perception and thinking are not capable of conveying reliable information about the world." None of these experiments disproved this. In fact, they confirmed it. Our hardware is untrustworthy.

But the most damning realization is that our **rational thinking** cannot override these errors. Even when you know the "X" is there, you can’t see it. Even when you know the squares are the same grey, they look different. Even when you know the hand is rubber, you feel it.

Why are we built this way? Because our thinking is the result of millions of years of evolution, and evolution does not care about "Truth." Evolution cares about "Survival." For the vast majority of our history, knowing the exact hex-code value of a grey pixel didn't matter. Recognizing a pattern quickly—identifying a tiger in the shadows—mattered.

We are designed to project **Meaning** (Sinn) onto the world. We are storytelling engines, constantly connecting dots to create a coherent narrative. We discussed this in Critical Thinking under the concept of the **"Cognitive Miser."** As Richerson & Boyd noted in 2005: "In effect, all animals are under stringent selection pressure to be as stupid as they can get away with." Processing power is expensive (calories), so the brain takes shortcuts. It biases towards meaning and speed, not accuracy.

### The Truth-Meaning Gap

This creates a "Meaning-Truth Scissors"—a divergence between what feels true (Meaning) and what is factually true (Truth). The brain’s strategy of projecting sense onto the world is so dominant that it interferes with our ability to observe the world as it actually is.

This leads us to the **Big Idea**—the "OG" concept of scientific thinking:

If we want to learn anything about the real world, we must **radically distrust our senses.** We cannot just be "careful." We cannot just "double-check." We have to operate on the assumption that our perception is actively lying to us. Nothing our senses tell us can be accepted as reliable data until it has been stripped of human interpretation.

Science, therefore, isn't just a collection of smart people looking at things. It is a rigorous methodology designed to bypass the human brain's inherent inability to see the truth.

---

_Next up: We travel back in time to see how Francis Bacon and a few forgotten global thinkers fought to build a system that could bypass these biological glitches._

## Part 2: Deconstructing the Narrative (History, Alchemy, and the Real Roots of Reason)

We established in the previous section that your brain is a survival engine, not a truth engine. If we want to get anywhere near objective reality, we need a system that assumes our senses are compromised. This realization didn't happen overnight, and it wasn't the product of a single "Eureka" moment. It was a slow, messy, and often politically dangerous evolution of thought.

To understand where our modern scientific operating system comes from, we have to look at the people who first started documenting the glitches in the human mind.

### The Four Idols of the Mind (Sir Francis Bacon)

In 1620, Francis Bacon published _Novum Organum_ (The New Instrument), essentially a user manual for bypassing human stupidity. Bacon identified four specific "Idols"—systematic errors or illusions—that prevent us from seeing the truth. These aren't physical statues; they are categories of cognitive failure.

First, he identified the **Idola Tribus (Idols of the Tribe)**. This refers to the errors inherent to human nature itself. This is exactly what we covered in Part 1: the biological limitations of our senses, our tendency to find patterns where none exist, and our evolutionary baggage. These are the bugs hardcoded into the firmware of the human species.

Second are the **Idola Specus (Idols of the Cave)**. These are the errors specific to the individual. We all grow up in a "cave"—our specific culture, upbringing, education, and reading habits. This personal context filters the light of reality, meaning no two people see the world exactly the same way because they are looking out from different caves.

Third, and perhaps most insidious, are the **Idola Fori (Idols of the Market)**. This refers to the "Marketplace of Ideas," or language itself. We use words to describe the world, but language is imprecise, loaded, and often defined by the masses, not the experts. We get trapped by definitions and semantics, arguing over words rather than the reality those words are supposed to represent. The framework of our language limits the framework of our thinking.

Finally, the **Idola Theatri (Idols of the Theater)**. These are the dogmas and received systems of philosophy. Bacon called them this because he viewed previous philosophical systems (like Aristotelian logic) as stage plays—artificial worlds created for entertainment or comfort that have no bearing on reality. It is the tendency to accept established academic or scientific authority without question, creating a resistance to fundamentally new ideas.

It is worth noting that while Bacon’s ideas were revolutionary, the man himself was … complicated. By all historical accounts, he was a fairly nasty piece of work. As Lord Chancellor, he justified corruption, was bribable himself, operated as a ruthless opportunist, and was generally disliked. But even a corrupt opportunist can spot a system failure. Bacon proposed a new method to escape these Idols: rather than testing assumptions until they are confirmed (confirmation bias), we should draw well-founded conclusions from experimental data. He explicitly argued for focusing on **negative instances**—evidence that disproves a theory—because that is where the real information hides. He rejected the Church’s stance that "all knowledge is already in the Bible" and demanded work that was **systematic** (repeatable), **empirical** (observation-based), and **inductive**.

### The Cartesian Cleanroom

Around the same time, in 1632, René Descartes took a slightly different but equally radical approach. He published _Discours de la méthode_ (Discourse on the Method), proposing a universal way to search for truth. If Bacon was about empirical data, Descartes was about rigorous logical hygiene.

His method was simple but brutal: accept nothing as true unless you can verify it yourself through analysis and logical reflection. He demanded **rigorous deduction**—deriving conclusions only from facts that are already proven—and championed mathematics as the only language reliable enough to formulate these insights. Mathematics, unlike spoken language (Bacon's _Idola Fori_), leaves little room for ambiguity.

The full title of his work is _Discourse on the Method of Rightly Conducting One's Reason and of Seeking Truth in the Sciences_. Interestingly, Descartes published this anonymously in Leiden, Netherlands. Why hide his name? Because in 1632, claiming you had a new method for finding truth that bypassed the Church was a good way to get in serious trouble. He was navigating a minefield of authority.

### Alchemy: The Introverted Cousin of Science

We often have a cartoonish view of the era preceding this "Scientific Revolution." We imagine Alchemists as crazy wizards trying to turn lead into gold or finding the Philosopher's Stone. This is a massive distortion. Alchemy was **proto-science**.

Alchemists were doing the heavy lifting of material science long before we had a periodic table. European alchemists (re)discovered porcelain and gunpowder. Isaac Newton, arguably the poster child of the scientific revolution, considered himself an alchemist and wrote extensively on the subject using terms like "spirits" and "active principles."

The problem with Alchemy wasn't the intelligence of the people involved; it was the protocol. Alchemy was obsessed with **secrecy**. The motivation wasn't public knowledge; it was private gain (gold, immortality, weaponizable powder). Therefore, alchemists worked in code, hid their findings, and never shared their data.

This secrecy was fatal to progress. Because no one shared their lab notes, no one could learn from anyone else's mistakes. Every alchemist had to start from scratch, committing the same errors over and over again. Science, as we define it today, only began when we inverted this dynamic. The shift from "Knowledge is Power (so hide it)" to "Knowledge is Progress (so share it)" was the critical software update.

### The Great Whitewashing: It Wasn't Just Europe

We love to tell the story that Europe "invented" science, drawing a straight line from the ancient Greeks to Bacon and Descartes. This is a fabrication of history. The sparks that ignited the scientific revolution didn't just fly in London and Paris; they were burning globally for centuries. The European narrative is the result of selective forgetting and, frankly, imperialist history writing.

Consider **Ibn Rushd** (1130–1198), a Cordoban scholar who argued that men and women possessed equal intellect and should have equal rights—a radical idea for the 12th century. He also argued that a concept could not be theologically true but philosophically false (and vice versa), laying the groundwork for separating faith from reason.

Then there is **Al-Haytham** (965–1040) from Cairo. If anyone deserves the title of "Father of the Scientific Method," it is him. Centuries before Bacon’s _Novum Organum_, Al-Haytham was running systematic experiments and formulating theories. He explicitly warned against the _Idols_ of authority long before Bacon gave them a name. In his writings, he stated that the seeker of truth is not the one who trusts the writings of the ancients, but the one who "suspects his faith in them" and "attacks [the text] from every side." He argued that to find the truth, one must make oneself an enemy of everything one reads.

Perhaps most striking is the story of **Zera Yacob** (1599–1693), an Ethiopian philosopher. Working from a cave in Ethiopia between 1630 and 1632 (exact contemporaries with Descartes), he developed a rationalist philosophy that mirrored the best of the European Enlightenment. He argued for the supremacy of reason, the equality of all humans (male and female), and the evil of slavery, and he critiqued established religions. He did this in isolation, far from the European salons.

Why do we not know these names? As historian Dag Herbjørnsrud points out, the "Age of Reason" in Europe coincided with an age of brutal imperialism. The West didn't win the world because its ideas were superior; it won because it was superior at organized violence. To justify colonization, the narrative of "Reason" became racially coded. The contributions of Africa and Asia were systematically erased to support the idea that rational thought was a uniquely white, European trait. This "closing of the European mind" in the 18th century cemented the false history we are often taught today.

### The Hardware Upgrade: The Printing Press & The Fight for Doubt

The final piece of the puzzle was a technological disruption: the **Printing Press**. Before print, knowledge was expensive and controlled. The Church and the State decided what was copied. The printing press broke this monopoly. It allowed for the dissemination of not just holy texts, but _any_ text.

This sparked a rebellion. In the mid-17th century, the first scientific societies (like the Royal Society in the UK and the French Academy of Sciences) were founded. These were essentially unions of researchers rebelling against their patrons. They argued that the goal should be advancing science through exchange, not just satisfying the whims of a funding lord. It was similar to the founding of "United Artists" in 1919, where filmmakers unionized to take control back from the studios.

This era defined the **First Pillar of Science: Doubt.**

Previously, Truth was the property of authority. If the Pope or the King said it, it was true. Questioning it was treason or heresy. Think of Galileo and Kepler, who were harassed and punished for doubting the geocentric model. But the printing press made doubt resilient. You couldn't burn all the books if they were being printed faster than you could find them. A scene developed of mobile printing presses—literally printing banned books on ships to evade jurisdiction—using censorship lists as market research to see what people wanted to read.

This solidified the rule: **Everything may be doubted.**

This is not easy. Doubt is uncomfortable. In a pragmatic sense, you can't doubt everything all the time (you wouldn't get a rocket to the moon if you spent every meeting debating if the earth is flat). But in principle, _nothing_ is off-limits. Religions often ban doubt; authoritarian regimes criminalize it; conspiracy theorists weaponize it (doubting the mainstream to enforce their own dogma). But in science, doubt is the engine. It is the refusal to accept "because I said so" as a valid argument.

---

_Next up: We break down the engine itself—how we took this philosophy of doubt and turned it into the rigorous, error-checking machine we call the Scientific Method._

## Part 3: The Engine of Truth (The Method, Hypotheses, and the Causality Trap)

We established in the previous sections that our brains are glitchy survival engines and that we need a culture of doubt to bypass authoritarian control. Now, we have to build the machine itself. To operationalize that doubt, early scientists—building on the legacy of Bacon and Descartes—formalized a specific set of heuristics to keep us honest.

The first move was to replace our biological senses with instruments. Since we cannot trust our eyes to judge brightness or our skin to judge temperature objectively, we build devices like thermometers, photometers, and rulers. These tools don't hallucinate, though they aren't magic; they are built by humans and can reflect human bias. The polygraph, or lie detector, is a perfect example of a broken instrument. It measures biological stress (sweat, heart rate), which has no proven correlation with truth-telling. It measures anxiety, not lies, yet we often treat the needle on the chart as an oracle.

We also shifted our language. Spoken languages are messy and loaded with cultural baggage—Bacon’s _Idols of the Market_. To escape this, science adopted mathematics as its core language. Math expresses very little in terms of emotion or nuance, but what it does express, it expresses without ambiguity. Finally, we agreed on a hierarchy of argument where measurable data and verified facts sit at the top, displacing opinion and authority.

### Defining the Indefinable

Before we break down the mechanics, we need a working definition of what we are actually doing. Science is best defined as the **continuous process** of describing the world in our symbolic representations—be that language, math, or code—such that the description withstands every critical examination.

It is critical to understand that science is not a static list of facts or a dusty encyclopedia. It is the act of refining a description until you can no longer poke holes in it. It is also not a belief system. Religion seeks salvation; science seeks insight. As the _Science Busters_ motto puts it, science is what holds true even if you don’t believe in it. To navigate this, philosophers use three specific coordinates. We ask what exists (Ontology), what we can actually know about it (Epistemology), and what that means for our values and ethics (Axiology).

### The Four Archetypes of the Method

To understand how this abstract definition hits the pavement, we can look at four historical examples that defined the "Classical Scientific Method."

First, consider **John Snow** during the 1854 London cholera outbreak. While the city was dying and officials blamed "miasma" (bad air), Snow suspected the water supply. He didn't just argue; he visualized. He took a map of SoHo and plotted a black bar for every death at a specific address. The visualization revealed a clustering of death around a single point: the water pump on Broadwick Street. He used this data to convince officials to remove the pump handle, stopping the outbreak. It was later confirmed a cesspit was leaking into the well. Snow used data visualization to make a hypothesis actionable.

Second, we have **Isaac Newton**. Still operating with the mindset of an alchemist, Newton was the first to model a physical force mathematically. He stood on the shoulders of Galileo, who had hypothesized that objects of different masses fall at the same speed if you ignore air resistance. This was visually proven centuries later during the Apollo 15 mission, when Commander David Scott dropped a geological hammer and a falcon feather on the airless lunar surface. They hit the dust at the exact same instant. Newton took this physical reality and encoded it into math that allowed us to predict planetary movement without interpretive wiggle room.

Third is **Fitts’s Law**, a cornerstone of Human-Computer Interaction established by psychologist Paul Fitts in 1954. He mathematically modeled human movement, discovering that the time required to move a pointer to a target is a function of the distance to the target divided by the size of the target. This is why the "OK" button on your software interface is usually large, and why the corners of your screen are considered "infinite width" targets because you can’t overshoot them with a mouse. It is a biological truth encoded into a formula ($ID = \log_2(2D/W)$).

Finally, **Alan Turing** proposed the "Imitation Game" in 1950 to handle the slippery definition of artificial intelligence. Since "intelligence" is hard to ontologically define, he operationalized it: if a machine can trick a human into thinking it is human via text chat, it passes. While we now know this test is flawed—ChatGPT can pass it without possessing understanding—it was a crucial attempt to turn a philosophical question into an experimental procedure.

### The Algorithm: The Classical Scientific Method

These examples reveal a specific loop which forms the second pillar of science: Systematic Procedure. The loop generally flows from observation to speculation, then to the formulation of a hypothesis, followed by an experiment, and finally a conclusion where the hypothesis is either accepted or rejected.

The **Hypothesis** is the critical firewall in this process. It stops us from simply telling stories about what we see. A scientific hypothesis is an inductive leap—a grounded guess that we assume is true until proven otherwise. However, not every statement qualifies. A hypothesis must be testable. Saying "There is an invisible entity controlling the universe" is a bad hypothesis because it cannot be tested. Saying "Cholera is caused by this specific water pump" is a good hypothesis because you can test it by removing the pump handle.

In statistics, we often use the **Null Hypothesis** ($H_0$), which effectively assumes "innocence until proven guilty." The Null Hypothesis states that there is no connection between two variables—for example, that video games do not cause violence. We hold this as true until the data overwhelmingly forces us to reject it. In the specific case of video games and violence, meta-studies by researchers like Prescott, Sargent, and Hull suggest we might technically reject the Null Hypothesis, but the effect size is so tiny compared to other factors that it is practically negligible.

There is a "dirty secret" here. The second step of the method—speculation and hypothesis generation—is the only part of science we don't know how to automate. It requires intuition and creativity. It is the one moment where creative thinking is the dominant force in the rigorous scientific process.

### Falsifiability and the Experiment

Once a hypothesis is set, we run into the harsh reality of logic introduced by philosopher Karl Popper. Popper argued that you can never prove a theory is true; you can only prove it is false. This concept is called **Falsifiability**. No matter how many white swans you see, you have not proven the statement "All swans are white." But seeing a single black swan definitively proves the statement false. Therefore, science isn't a collection of truths; it is a collection of statements that haven't been proven wrong _yet_.

To try and falsify our hypotheses, we run experiments where we manipulate reality. We identify an **Independent Variable** (the thing we change, like a game version) and measure a **Dependent Variable** (the result, like a player's heart rate). Crucially, we must hold everything else constant as **Control Variables**. If you change the game version _and_ the age of the test subject at the same time, the experiment is ruined. You can no longer tell which variable caused the change in heart rate. This seems like basic hygiene, yet it is a frequent point of failure in research.

### The Trap: Correlation vs. Causation

Interpreting experimental data brings us to the most common glitch in human reasoning: confusing correlation with causation. **Correlation** describes a mathematical relationship where two values move together. **Causation** means one thing actually triggers the other.

Our brains love to assume causation. If the temperature of a heater goes up, the room temperature goes up; that is causal. But often, the world offers us **Spurious Correlations**. Tyler Vigen created a famous website demonstrating this by graphing variables that match perfectly but have no connection. For instance, the graph showing the distance of Jupiter from the Sun matches the graph showing the number of administrative assistants in Alaska almost perfectly. The math says they are related; reality says that is nonsense.

A more subtle example is the **Uncanny Valley**. In 1970, Masahiro Mori hypothesized that as robots become more human-like, our empathy increases, until they get _too_ close and we feel revulsion. For decades, this was just a correlation based on observation. It wasn't until 2016 that researchers began to experimentally prove the _causality_ behind it, linking it to categorization failure in the brain. Until that causal link is forged, a correlation is just a coincidence waiting to be debunked.

### Theory: The Holy Grail

When a hypothesis survives enough attempts to kill it, it graduates to a **Theory**. A theory is a consolidated explanation of a slice of reality. A good theory, like Einstein’s Theory of Relativity, solves open questions and predicts things we haven't seen yet—like black holes—without falling apart under testing.

But theories can also be traps. In the late 19th century, the accepted theory of aerodynamics suggested that heavier-than-air flight was effectively impossible or impractical. This was a "bad theory," but it was socially accepted. The Wright Brothers succeeded because they chose to actively doubt the theory. They built their own wind tunnel, generated their own data, and ignored the scientific consensus. Theories are cognitive artifacts that let us think new thoughts, but they are also social artifacts. If the Wright Brothers hadn't flown, the bad theory of aerodynamics might have persisted for another fifty years simply because the experts agreed on it.

---

_Next up: We enter the messy world of "Paradigms"—why the rules of science change depending on whether you are studying an iron cube or a group of nuns._

## Part 4: Whose Truth Is It Anyway? (Paradigms, Bias, and the Limits of Objectivity)

We have built the machine (the Scientific Method), but now we have to decide where to point it. This brings us to the concept of **Paradigms**. A paradigm is essentially a framework of beliefs and assumptions that defines how we look at the world. It dictates what questions we are allowed to ask and what kind of answers we accept as valid.

The scientific revolution ushered in the age of **Modernism**. This was the era of the Industrial Revolution, where we began to view the world as an objective, measurable reality and ourselves as rational, complex machines. It birthed a kind of "techno-solutionism"—the belief that any problem, no matter how human, could be solved with enough engineering. You can see this optimism in the art of the time, like Italian Futurism, which worshipped speed and machinery. But you also see the anxiety in early science fiction. H.G. Wells and Aldous Huxley wrote warnings, but the most striking was perhaps Yevgeny Zamyatin’s 1920 novel _We_, which depicted a dystopian world ruled entirely by cold mathematical logic.

### The Reign of Positivism

The dominant operating system of Modernism was **Positivism**. This view holds that the world exists entirely independently of us. It is out there, solid and separate. Therefore, knowledge can only be derived from actual, verifiable phenomena. In its most radical form, Positivism assumes that _everything_ in reality can be described mathematically.

This works incredibly well for physics. If you want to study an iron cube, Positivism is your friend. You can measure its weight, density, and temperature. The cube doesn’t care that you are measuring it, and your feelings about the cube don't change its mass. But this approach hits a wall when you try to study a human brain or a society. As researchers Paley and Litford argued in 2011, reality isn't actually fragmented into variables; variables are just things we invented to measure reality. When we reduce a human being to a set of numbers, we are making a choice to leave things out.

### The Blind Spot of Objectivity: The Viking Warrior

The failure of strict Positivism becomes obvious when we look at how "objective" scientists handle history. In 1878, archaeologists in Sweden found a Viking grave (Bj 581) in Birka. The grave was loaded with weapons and gaming pieces, which indicated high tactical and military rank. The scientific consensus was immediate and absolute: this was a high-ranking male warrior. For over a century, this was the "truth."

It wasn't until 2017 that a genomic analysis revealed the skeleton was XX. Female. The scientific community had been looking at a skeleton with wide hips, but because their internal paradigm said "Warriors are Men," they literally couldn't see the biological data in front of them. They assumed the weapons were ceremonial or heirlooms. Positivism claimed to be value-free, but it had no mechanism to account for the sexist bias of the researchers. The Axiology (values) of Positivism says "values don't matter," which ironically allows the researcher's values to contaminate the data unchecked.

### Post-Positivism: The "Yes, But" Approach

As the cracks in Positivism showed, we moved toward **Post-Positivism**. This framework accepts that an objective reality exists, but admits we can never see it perfectly. It relies heavily on Karl Popper’s insight that observation is always selective. You cannot just "observe"; you always observe _something_ based on your interests and theories.

Post-Positivism trades certainty for probability. It dominates fields like medicine and experimental psychology. A great example of why this is necessary is the study of eHealth systems. Researchers Greenhalgh and Russell (2010) found that digital health tools often worked perfectly in controlled trials (the Positivist "Lab" view) but failed miserably in real hospitals. The chaotic reality of a hospital introduced variables that the "clean" science had stripped away. Post-Positivism tries to account for this context, admitting that while the truth is out there, we are looking at it through a foggy window.

### Critical Theory: It’s All About Power

If Post-Positivism is an update to the old system, **Critical Theory** is a jailbreak. This paradigm isn't interested in just describing the world; it wants to know _why_ the world is the way it is, usually focusing on power structures. It argues that science is never neutral. As early computer scientist Joe Weizenbaum pointed out, the choice of _which_ questions we ask is itself a value judgment.

In this view, knowledge is "co-constructed" between the researcher and the participant. A classic example is the "Prayer Companion" designed by Bill Gaver. Instead of just building a "user-friendly" device for nuns, the team investigated the specific spiritual needs of a convent. The device scrolled news headlines so the nuns could direct their prayers toward current world suffering. A Positivist might ask "Is the device efficient?" Critical Theory asks "Does this device empower the nuns within their specific cultural context?"

### Constructivism: Reality is a Gentleman’s Agreement

Finally, we have **Constructivism**. This paradigm argues that much of what we consider "reality" is just a social agreement. An iron cube is real, but money? The Supreme Court? Gender roles? Those are social constructs. They only exist because we agree they exist.

When we ignore this, we fail. Consider the digitalization of hospital forms. A study by Geraldine Fitzpatrick showed that paper forms in hospitals weren't just data containers. Nurses scribbled notes in the margins; they used the physical location of the clipboard to signal status; they used the paper as a communication tool in ways not defined by the "fields." When IT consultants tried to digitize this by simply turning text fields into database rows, the system failed. They captured the "data" (Positivism) but destroyed the "social construction" of the work (Constructivism).

### The Cockpit: A Grand Unification

So, which paradigm is the "correct" one? This is a childish question. Good science can be done under any paradigm, provided you stick to the three pillars: Doubt, Method, and Openness. The choice of paradigm depends entirely on the question you need to answer.

Let’s look at an airplane cockpit.

If you are a **Positivist**, you ask: "Does the altimeter provide the correct altitude within 0.1% accuracy?" You are testing hardware and software correctness. This is vital.

If you are a **Critical Theorist**, you ask: "Why is the cockpit designed this way? Does it favor a certain type of pilot (e.g., male, Western)? Does the hierarchy between the pilot and co-pilot prevent safety checks?"

If you are a **Constructivist**, you ask: "How do the pilots feel about the automation? Do they trust it? How does the culture of the airline influence how they use the autopilot?"

For a long time, aviation focused heavily on the Positivist view (engineering). But a 2013 FAA study found that this focus created a new danger: pilots were becoming over-reliant on automation. When the systems failed, the pilots didn't know how to fly the plane manually. The pure engineering approach missed the human factor. By ignoring the Constructivist questions (how do humans interact with the machine?), the industry created a new class of accidents.

We need all the lenses. If you only have a hammer, every problem looks like a nail. If you only have Positivism, every problem looks like a math equation. And sometimes, the problem is a Viking woman, a nun, or a pilot who has forgotten how to fly.

---

_Next up: The ugly truth about the academic industry—fraud, the "Publish or Perish" meat grinder, and how to survive it all with your integrity intact._

## Part 5: The Academic Game & How to Play It (Fraud, Metrics, and Practical Wisdom)

We have spent four sections building up the ideal of science: a noble, rigorous pursuit of truth built on doubt and systematic checking. Now, we have to look at the reality of the _industry_ of science. Because science is done by humans, and humans are prone to greed, laziness, and gaming the system, the "Temple of Reason" has a fair amount of rot in the foundations.

### The Hall of Shame: When Science is Faked

Between the ideal of the Scientific Method and the reality of a career in research, a gray zone emerges. Sometimes, it’s not just gray; it is pitch black. Take James Mellaart, a celebrated British archaeologist. For decades, he was a titan in the field, discovering Neolithic sites in Turkey. It wasn't until he died in 2012 that researchers went through his archives and realized he had been systematically faking sketches and "finds" since the 1960s to support his theories. He played the long con, and because of his authority, nobody checked the receipts while he was alive.

But sometimes the fraud is designed to expose the system, not exploit it. In 1994, Austrian researchers Werner Purgathofer, Eduard Gröller, and Martin Feda suspected that the _VIDEA '95_ conference had zero quality control. To prove it, they submitted four completely absurd abstracts filled with subversive humor and technical nonsense. All four were accepted. They went public with the "Beware of VIDEA!" manifesto to shame the organizers.

This tradition of "sting operations" continued. In 2005, three MIT students built _SCIgen_, a software that automatically generates grammatically correct but meaningless computer science papers. They submitted a paper titled "Rooter: A Methodology for the Typical Unification of Access Points and Redundancy" to a conference, and it was accepted. In 2009, Philip Davis from Cornell used similar software to submit a paper to _The Open Information Science Journal_. The journal accepted it, asking only for an $800 publication fee. They didn't care about the science; they cared about the check.

The most recent and grotesque example dropped in 2024, when the journal _Frontiers in Cell Development and Biology_—supposedly a peer-reviewed publication—published a paper containing AI-generated diagrams. One diagram featured a rat with a biologically impossible, gargantuan reproductive organ, labeled with gibberish text like "dck." The image went viral on social media, the paper was retracted, and the reviewers claimed it "wasn't their job" to check the images. These aren't just funny anecdotes; they are structural failures showing that the "critical collective review" we rely on is often asleep at the wheel.

### The Grind: Publish or Perish

Why is the quality control failing? Because the economic incentives of science are broken. We call this culture **Publish or Perish**. In the academic world, published papers are currency. You pay for your career advancements, grants, and tenure with PDFs.

This creates a pressure cooker. Instead of spending five years working on one major breakthrough, researchers are incentivized to slice their work into the smallest possible "publishable units" (salami slicing) to get three papers out of one experiment. Even worse, the business model is bizarre: Scientists do the research (paid by taxpayers), write the papers (for free), review other scientists' papers (for free), and edit the journals (for free). Then, for-profit publishers take this content and sell it back to the universities for exorbitant subscription fees.

### The Scoreboard: The h-Index and Goodhart’s Law

To make matters worse, administrators love to quantify "excellence." They want a number that tells them if a scientist is "good." The most common metric is the **h-Index**, proposed by Jorge Hirsch. Your h-Index is $X$ if you have $X$ papers that have been cited at least $X$ times. It sounds objective, but it turns science into a video game.

If you tell a smart person how the score is calculated, they will optimize for the score, not the science. This is **Goodhart’s Law**: "When a measure becomes a target, it ceases to be a good measure." Researchers can game their h-Index by citing themselves relentlessly or by writing "Review Papers" (summaries of other people's work) which get cited often but add no new knowledge.

Marc A. Edwards and Siddhartha Roy analyzed this in 2017, documenting the "Perverse Incentives" of academia. The system rewards quantity over quality. As astrophysicist Paul M. Sutter notes in his book _Rescuing Science_, the current incentive is "Keep publishing, dang it." There is zero incentive to do careful peer reviews, replicate results (which is boring and gets no citations), or publish negative results. The system is designed to produce noise, not truth.

### Praxistipps: How to Think Like a Scientist (Without Losing Your Soul)

Despite the systemic issues, the core toolkit of scientific thinking is the most powerful mental software you can install. Here is how to apply it to your actual life and career.

**1. Cultivate Functional Doubt**
The first tip is the hardest: Learn to doubt. But you must distinguish between **Functional Doubt** (doubting ideas) and **Dysfunctional Doubt** (doubting yourself). Functional doubt drives you to check sources and test assumptions. Dysfunctional doubt (Identity Doubt) paralyzes you. You should doubt your hypothesis, but do not let that curdle into Imposter Syndrome where you doubt your ability to think.

**2. The Receipt Strategy (Sources)**
When you are researching or arguing a point, look for sources that confirm your view _and_ sources that contradict it. If you can't find a source that disagrees with you, you aren't looking hard enough. Also, drop the ego about "originality." Extensive citing isn't cheating; it's proof of quality. It shows you have done the work to map the territory.

**3. Discourse Control**
When someone attacks your idea, your lizard brain thinks they are attacking _you_. You will feel the urge to get defensive. Fight this. Count to ten. Science happens in the **Discourse**—the cool, detached exchange of arguments—not in a heated debate. If you get emotional, you have lost the ability to think critically.

**4. Scientific Reasoning (Induction vs. Deduction)**
Understand the difference in how you build arguments.

- **Induction** is bottom-up. You see 100 specific cases and guess a general rule. (e.g., "I've only seen dangerous drivers on this street, so this street is dangerous.") This is useful but probabilistic. It generates hypotheses.
- **Deduction** is top-down. You take established facts and logically derive a conclusion. (e.g., "All men are mortal. Socrates is a man. Therefore, Socrates is mortal.") This is logically sound, provided your premises are correct.

**5. The Final Warning: Correlation is still not Causation**
We end where we began. Your brain is a pattern-matching machine that wants to connect dots. It sees a correlation (e.g., "I washed my car, and then it rained") and invents a cause ("Washing the car causes rain"). Fight this instinct. Always assume a correlation is a coincidence until you have a mechanism to explain the causality.

### Summary

Scientific Thinking is simply a systematic, traceable form of curiosity. It requires admitting that we are easily fooled, that our senses are flawed, and that our biases are strong. It demands that we build systems—peer review, open data, falsification—to protect us from our own nature. Whether you are debugging code, diagnosing a patient, or just reading the news, the goal is the same: to describe the world in a way that withstands critical scrutiny.

---

_End of Series. That was it!_
